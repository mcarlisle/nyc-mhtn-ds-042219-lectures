{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 2, \n",
    "                           alpha = 1, \n",
    "                           n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784753\n",
      "F1: 0.675676\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds**: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael/anaconda3/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/Users/michael/anaconda3/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 2, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.640294</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.640921</td>\n",
       "      <td>0.011085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.629094</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.630053</td>\n",
       "      <td>0.011036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.616500</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.617743</td>\n",
       "      <td>0.016020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.601325</td>\n",
       "      <td>0.011162</td>\n",
       "      <td>0.602671</td>\n",
       "      <td>0.015340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.590203</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>0.592608</td>\n",
       "      <td>0.015515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.580331</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>0.583432</td>\n",
       "      <td>0.011188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.570725</td>\n",
       "      <td>0.015010</td>\n",
       "      <td>0.574913</td>\n",
       "      <td>0.009321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.564326</td>\n",
       "      <td>0.016661</td>\n",
       "      <td>0.569265</td>\n",
       "      <td>0.009131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.557697</td>\n",
       "      <td>0.015285</td>\n",
       "      <td>0.563021</td>\n",
       "      <td>0.007609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.552730</td>\n",
       "      <td>0.015043</td>\n",
       "      <td>0.558481</td>\n",
       "      <td>0.007250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.547068</td>\n",
       "      <td>0.015693</td>\n",
       "      <td>0.552817</td>\n",
       "      <td>0.008987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.542089</td>\n",
       "      <td>0.015239</td>\n",
       "      <td>0.548452</td>\n",
       "      <td>0.011089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.536571</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.543687</td>\n",
       "      <td>0.011051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.531027</td>\n",
       "      <td>0.013537</td>\n",
       "      <td>0.538226</td>\n",
       "      <td>0.012720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.528377</td>\n",
       "      <td>0.012291</td>\n",
       "      <td>0.535746</td>\n",
       "      <td>0.012291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.523079</td>\n",
       "      <td>0.008863</td>\n",
       "      <td>0.531756</td>\n",
       "      <td>0.013968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.518823</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.527936</td>\n",
       "      <td>0.016220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.516713</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.525689</td>\n",
       "      <td>0.017301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.512552</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.521564</td>\n",
       "      <td>0.015942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.518616</td>\n",
       "      <td>0.013734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.503175</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.513420</td>\n",
       "      <td>0.017039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.500755</td>\n",
       "      <td>0.003567</td>\n",
       "      <td>0.511212</td>\n",
       "      <td>0.017609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.498579</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.509316</td>\n",
       "      <td>0.019612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.497073</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.508617</td>\n",
       "      <td>0.020337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.493866</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.505923</td>\n",
       "      <td>0.018245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.492352</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.505034</td>\n",
       "      <td>0.018548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.488268</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.501230</td>\n",
       "      <td>0.021190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.485094</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.498670</td>\n",
       "      <td>0.023099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.483280</td>\n",
       "      <td>0.008005</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.024599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.405556</td>\n",
       "      <td>0.010567</td>\n",
       "      <td>0.438868</td>\n",
       "      <td>0.033639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.405416</td>\n",
       "      <td>0.010647</td>\n",
       "      <td>0.438799</td>\n",
       "      <td>0.033762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.405035</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.438441</td>\n",
       "      <td>0.034176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.404769</td>\n",
       "      <td>0.010535</td>\n",
       "      <td>0.438322</td>\n",
       "      <td>0.034408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.404579</td>\n",
       "      <td>0.010568</td>\n",
       "      <td>0.438402</td>\n",
       "      <td>0.034433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.404267</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.438157</td>\n",
       "      <td>0.034757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.403527</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.437963</td>\n",
       "      <td>0.034813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.403150</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.437628</td>\n",
       "      <td>0.034788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.402852</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.437338</td>\n",
       "      <td>0.035082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.402380</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.437535</td>\n",
       "      <td>0.035311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.402135</td>\n",
       "      <td>0.010734</td>\n",
       "      <td>0.437321</td>\n",
       "      <td>0.035330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.401965</td>\n",
       "      <td>0.010690</td>\n",
       "      <td>0.437123</td>\n",
       "      <td>0.035649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.401602</td>\n",
       "      <td>0.010874</td>\n",
       "      <td>0.437043</td>\n",
       "      <td>0.036012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.401007</td>\n",
       "      <td>0.011147</td>\n",
       "      <td>0.436706</td>\n",
       "      <td>0.035821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.400863</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>0.436732</td>\n",
       "      <td>0.035924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.400192</td>\n",
       "      <td>0.011175</td>\n",
       "      <td>0.436602</td>\n",
       "      <td>0.036186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.399852</td>\n",
       "      <td>0.011498</td>\n",
       "      <td>0.436334</td>\n",
       "      <td>0.035925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.399639</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>0.436299</td>\n",
       "      <td>0.035879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.399345</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>0.436111</td>\n",
       "      <td>0.035887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.399158</td>\n",
       "      <td>0.011383</td>\n",
       "      <td>0.436208</td>\n",
       "      <td>0.036155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.398955</td>\n",
       "      <td>0.011392</td>\n",
       "      <td>0.436230</td>\n",
       "      <td>0.036228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.398512</td>\n",
       "      <td>0.011461</td>\n",
       "      <td>0.436171</td>\n",
       "      <td>0.036488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.398139</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.436183</td>\n",
       "      <td>0.036983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.397733</td>\n",
       "      <td>0.011659</td>\n",
       "      <td>0.435879</td>\n",
       "      <td>0.037050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.397330</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.435761</td>\n",
       "      <td>0.037144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.396885</td>\n",
       "      <td>0.011926</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0.037383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.396632</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.435757</td>\n",
       "      <td>0.037410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.396027</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.435443</td>\n",
       "      <td>0.037449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.395488</td>\n",
       "      <td>0.011790</td>\n",
       "      <td>0.435333</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.394918</td>\n",
       "      <td>0.011901</td>\n",
       "      <td>0.435028</td>\n",
       "      <td>0.037540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000850           0.660168   \n",
       "1              0.640294           0.007082           0.640921   \n",
       "2              0.629094           0.006565           0.630053   \n",
       "3              0.616500           0.011333           0.617743   \n",
       "4              0.601325           0.011162           0.602671   \n",
       "5              0.590203           0.014328           0.592608   \n",
       "6              0.580331           0.011428           0.583432   \n",
       "7              0.570725           0.015010           0.574913   \n",
       "8              0.564326           0.016661           0.569265   \n",
       "9              0.557697           0.015285           0.563021   \n",
       "10             0.552730           0.015043           0.558481   \n",
       "11             0.547068           0.015693           0.552817   \n",
       "12             0.542089           0.015239           0.548452   \n",
       "13             0.536571           0.017683           0.543687   \n",
       "14             0.531027           0.013537           0.538226   \n",
       "15             0.528377           0.012291           0.535746   \n",
       "16             0.523079           0.008863           0.531756   \n",
       "17             0.518823           0.006483           0.527936   \n",
       "18             0.516713           0.006924           0.525689   \n",
       "19             0.512552           0.004859           0.521564   \n",
       "20             0.508500           0.005185           0.518616   \n",
       "21             0.503175           0.002684           0.513420   \n",
       "22             0.500755           0.003567           0.511212   \n",
       "23             0.498579           0.004905           0.509316   \n",
       "24             0.497073           0.005650           0.508617   \n",
       "25             0.493866           0.006596           0.505923   \n",
       "26             0.492352           0.006470           0.505034   \n",
       "27             0.488268           0.004673           0.501230   \n",
       "28             0.485094           0.007452           0.498670   \n",
       "29             0.483280           0.008005           0.497143   \n",
       "..                  ...                ...                ...   \n",
       "100            0.405556           0.010567           0.438868   \n",
       "101            0.405416           0.010647           0.438799   \n",
       "102            0.405035           0.010488           0.438441   \n",
       "103            0.404769           0.010535           0.438322   \n",
       "104            0.404579           0.010568           0.438402   \n",
       "105            0.404267           0.010597           0.438157   \n",
       "106            0.403527           0.010805           0.437963   \n",
       "107            0.403150           0.010858           0.437628   \n",
       "108            0.402852           0.010817           0.437338   \n",
       "109            0.402380           0.010784           0.437535   \n",
       "110            0.402135           0.010734           0.437321   \n",
       "111            0.401965           0.010690           0.437123   \n",
       "112            0.401602           0.010874           0.437043   \n",
       "113            0.401007           0.011147           0.436706   \n",
       "114            0.400863           0.011151           0.436732   \n",
       "115            0.400192           0.011175           0.436602   \n",
       "116            0.399852           0.011498           0.436334   \n",
       "117            0.399639           0.011438           0.436299   \n",
       "118            0.399345           0.011369           0.436111   \n",
       "119            0.399158           0.011383           0.436208   \n",
       "120            0.398955           0.011392           0.436230   \n",
       "121            0.398512           0.011461           0.436171   \n",
       "122            0.398139           0.011486           0.436183   \n",
       "123            0.397733           0.011659           0.435879   \n",
       "124            0.397330           0.011819           0.435761   \n",
       "125            0.396885           0.011926           0.435700   \n",
       "126            0.396632           0.011839           0.435757   \n",
       "127            0.396027           0.011676           0.435443   \n",
       "128            0.395488           0.011790           0.435333   \n",
       "129            0.394918           0.011901           0.435028   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001478  \n",
       "1            0.011085  \n",
       "2            0.011036  \n",
       "3            0.016020  \n",
       "4            0.015340  \n",
       "5            0.015515  \n",
       "6            0.011188  \n",
       "7            0.009321  \n",
       "8            0.009131  \n",
       "9            0.007609  \n",
       "10           0.007250  \n",
       "11           0.008987  \n",
       "12           0.011089  \n",
       "13           0.011051  \n",
       "14           0.012720  \n",
       "15           0.012291  \n",
       "16           0.013968  \n",
       "17           0.016220  \n",
       "18           0.017301  \n",
       "19           0.015942  \n",
       "20           0.013734  \n",
       "21           0.017039  \n",
       "22           0.017609  \n",
       "23           0.019612  \n",
       "24           0.020337  \n",
       "25           0.018245  \n",
       "26           0.018548  \n",
       "27           0.021190  \n",
       "28           0.023099  \n",
       "29           0.024599  \n",
       "..                ...  \n",
       "100          0.033639  \n",
       "101          0.033762  \n",
       "102          0.034176  \n",
       "103          0.034408  \n",
       "104          0.034433  \n",
       "105          0.034757  \n",
       "106          0.034813  \n",
       "107          0.034788  \n",
       "108          0.035082  \n",
       "109          0.035311  \n",
       "110          0.035330  \n",
       "111          0.035649  \n",
       "112          0.036012  \n",
       "113          0.035821  \n",
       "114          0.035924  \n",
       "115          0.036186  \n",
       "116          0.035925  \n",
       "117          0.035879  \n",
       "118          0.035887  \n",
       "119          0.036155  \n",
       "120          0.036228  \n",
       "121          0.036488  \n",
       "122          0.036983  \n",
       "123          0.037050  \n",
       "124          0.037144  \n",
       "125          0.037383  \n",
       "126          0.037410  \n",
       "127          0.037449  \n",
       "128          0.037500  \n",
       "129          0.037540  \n",
       "\n",
       "[130 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup our parameters for XGBoost to test\n",
    "param_test1 = {\n",
    " 'max_depth':range(2,9,2),\n",
    " 'min_child_weight':range(1,6,2),\n",
    " 'colsample_bytree': [0.2, 0.3, 0.4, 0.5],\n",
    " 'learning_rate': [0.05, 0.1, 0.2, 0.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = xgb.XGBClassifier( n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27),\n",
    "    param_grid = param_test1, \n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    iid=False, \n",
    "    cv=5,\n",
    "    verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 592 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:   26.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=140, n_jobs=1,\n",
       "       nthread=4, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
       "       subsample=0.8, verbosity=1),\n",
       "       fit_params=None, iid=False, n_jobs=-1,\n",
       "       param_grid={'max_depth': range(2, 9, 2), 'min_child_weight': range(1, 6, 2), 'colsample_bytree': [0.2, 0.3, 0.4, 0.5], 'learning_rate': [0.05, 0.1, 0.2, 0.3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/michael/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/michael/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/michael/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/michael/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/michael/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/michael/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.09192944, 0.11287107, 0.07887239, 0.08845057, 0.08981137,\n",
       "        0.07367058, 0.06906366, 0.07111855, 0.06874599, 0.08229308,\n",
       "        0.07136946, 0.06953201, 0.06860361, 0.06489892, 0.06656823,\n",
       "        0.06869378, 0.07473941, 0.07403574, 0.08316293, 0.074369  ,\n",
       "        0.07176981, 0.08504591, 0.07358236, 0.0711606 , 0.06475763,\n",
       "        0.06779528, 0.06869178, 0.06224937, 0.05808005, 0.05838718,\n",
       "        0.06887279, 0.06410494, 0.06489816, 0.06323318, 0.06714888,\n",
       "        0.06120219, 0.05660243, 0.05736504, 0.06305633, 0.06580896,\n",
       "        0.06698437, 0.06235051, 0.06354461, 0.07105536, 0.06622639,\n",
       "        0.06704822, 0.06758418, 0.06377659, 0.06807461, 0.06589785,\n",
       "        0.06182346, 0.1049891 , 0.11168313, 0.1837574 , 0.0828846 ,\n",
       "        0.0860764 , 0.1001411 , 0.1086791 , 0.1024487 , 0.08979678,\n",
       "        0.06683669, 0.0634624 , 0.06746759, 0.07596235, 0.07660894,\n",
       "        0.07658086, 0.08278856, 0.08598785, 0.08073292, 0.09655476,\n",
       "        0.08707881, 0.08338442, 0.05987163, 0.06794195, 0.06730461,\n",
       "        0.07709103, 0.08091154, 0.07944722, 0.08551841, 0.08589087,\n",
       "        0.07889633, 0.08727851, 0.09102545, 0.08314958, 0.06801634,\n",
       "        0.11916118, 0.1414506 , 0.13181515, 0.07859077, 0.10577102,\n",
       "        0.15716257, 0.12714286, 0.07742085, 0.11876669, 0.15775661,\n",
       "        0.11956935, 0.08550673, 0.13155227, 0.10626173, 0.12633462,\n",
       "        0.09708562, 0.11558638, 0.14566822, 0.12208281, 0.09781857,\n",
       "        0.11078057, 0.10802817, 0.0957756 , 0.07284026, 0.07228408,\n",
       "        0.07570748, 0.09143295, 0.08531218, 0.09026995, 0.11906567,\n",
       "        0.10791616, 0.10370135, 0.11066227, 0.1034915 , 0.09393969,\n",
       "        0.06964293, 0.0863749 , 0.08316975, 0.08714943, 0.08208017,\n",
       "        0.09072475, 0.10390139, 0.09119954, 0.09272413, 0.11466818,\n",
       "        0.10328989, 0.08877926, 0.07481785, 0.06756682, 0.06705813,\n",
       "        0.0854898 , 0.088235  , 0.08427558, 0.1082005 , 0.09023685,\n",
       "        0.09224391, 0.11190343, 0.09836426, 0.08748603, 0.07645059,\n",
       "        0.07379165, 0.07305846, 0.09886045, 0.10022659, 0.09327993,\n",
       "        0.12438102, 0.11110353, 0.10295973, 0.13250136, 0.11464906,\n",
       "        0.10933108, 0.074019  , 0.07630911, 0.06871448, 0.0895926 ,\n",
       "        0.10063734, 0.10198836, 0.11260819, 0.11207156, 0.10633311,\n",
       "        0.14350939, 0.11443052, 0.09854808, 0.07110014, 0.07039003,\n",
       "        0.07371292, 0.10061617, 0.10023451, 0.0933342 , 0.11578708,\n",
       "        0.10841575, 0.10161486, 0.12992325, 0.11605053, 0.10924497,\n",
       "        0.07675147, 0.07757382, 0.07584877, 0.10073147, 0.0956409 ,\n",
       "        0.09622693, 0.12025304, 0.11213837, 0.09829469, 0.1231945 ,\n",
       "        0.09598346, 0.08232045]),\n",
       " 'std_fit_time': array([0.03254557, 0.03635474, 0.01542166, 0.01625859, 0.02312629,\n",
       "        0.02121367, 0.01372107, 0.00568629, 0.0066416 , 0.01855404,\n",
       "        0.00589541, 0.00283701, 0.02090084, 0.006705  , 0.00526768,\n",
       "        0.00568338, 0.00645387, 0.00701464, 0.00742544, 0.01539314,\n",
       "        0.00870808, 0.01396155, 0.01417112, 0.01284554, 0.01082167,\n",
       "        0.00527969, 0.01199779, 0.00751056, 0.00502393, 0.00694308,\n",
       "        0.00354026, 0.00483349, 0.00519775, 0.00173115, 0.00525408,\n",
       "        0.00534679, 0.00862393, 0.01240893, 0.00663465, 0.00320733,\n",
       "        0.00594466, 0.00822817, 0.00368652, 0.00666048, 0.00440888,\n",
       "        0.00553991, 0.00374707, 0.00284325, 0.00480605, 0.00165518,\n",
       "        0.00254539, 0.05359575, 0.05361367, 0.13011134, 0.01261451,\n",
       "        0.01200044, 0.01136543, 0.03302796, 0.01920463, 0.00965905,\n",
       "        0.00267575, 0.00149032, 0.00484239, 0.00743106, 0.00954051,\n",
       "        0.00961905, 0.00421819, 0.00491089, 0.00829628, 0.00410621,\n",
       "        0.00344309, 0.00388232, 0.01071869, 0.00655238, 0.00715374,\n",
       "        0.00933613, 0.00982376, 0.00599532, 0.00717855, 0.00352457,\n",
       "        0.00581283, 0.0015884 , 0.00658365, 0.00381947, 0.00677258,\n",
       "        0.05812322, 0.06132545, 0.08710527, 0.00640007, 0.07289516,\n",
       "        0.0910689 , 0.08515364, 0.01744812, 0.05106443, 0.06235593,\n",
       "        0.0357089 , 0.01151491, 0.05017954, 0.06329082, 0.05823723,\n",
       "        0.01460338, 0.04838554, 0.03897317, 0.0321861 , 0.00322274,\n",
       "        0.0032775 , 0.00783636, 0.00237039, 0.00465332, 0.00428821,\n",
       "        0.00396167, 0.00698238, 0.00394829, 0.00863661, 0.02316394,\n",
       "        0.01635781, 0.01164681, 0.0143171 , 0.00294208, 0.00208236,\n",
       "        0.00222362, 0.02296375, 0.02419501, 0.00912822, 0.00733439,\n",
       "        0.00315062, 0.0055435 , 0.00708623, 0.00457866, 0.00444043,\n",
       "        0.00658736, 0.00453982, 0.00579169, 0.00379675, 0.01028546,\n",
       "        0.00403207, 0.00317167, 0.00999025, 0.00571918, 0.00408629,\n",
       "        0.00556804, 0.00746969, 0.00969414, 0.00621978, 0.00128692,\n",
       "        0.00409414, 0.00186916, 0.00606768, 0.00363054, 0.00530307,\n",
       "        0.00756614, 0.00648347, 0.00871774, 0.0071051 , 0.00414847,\n",
       "        0.00612972, 0.00504155, 0.00379519, 0.00181589, 0.00984127,\n",
       "        0.00832612, 0.01871518, 0.00415144, 0.00631164, 0.00613911,\n",
       "        0.01045373, 0.00299482, 0.00693426, 0.00251154, 0.00560906,\n",
       "        0.00376687, 0.00499201, 0.00550185, 0.00614388, 0.00619313,\n",
       "        0.0059213 , 0.01035203, 0.00969357, 0.00578532, 0.00326269,\n",
       "        0.00518962, 0.00475371, 0.00710096, 0.00564652, 0.0031643 ,\n",
       "        0.00464609, 0.00482091, 0.00707645, 0.00364632, 0.00354766,\n",
       "        0.00942964, 0.00798605]),\n",
       " 'mean_score_time': array([0.02062116, 0.00889692, 0.00537562, 0.00599966, 0.0101388 ,\n",
       "        0.00502338, 0.00416045, 0.00567608, 0.00469184, 0.00649996,\n",
       "        0.00633755, 0.00614996, 0.00622797, 0.00479136, 0.00481071,\n",
       "        0.00520821, 0.00445399, 0.00533586, 0.00496764, 0.0043736 ,\n",
       "        0.00541816, 0.00437994, 0.00427637, 0.00700426, 0.00519047,\n",
       "        0.00401578, 0.0035676 , 0.00401525, 0.00363588, 0.00346208,\n",
       "        0.0054388 , 0.00480304, 0.0037086 , 0.00411797, 0.0038548 ,\n",
       "        0.00414639, 0.00380607, 0.00486293, 0.00473104, 0.00387807,\n",
       "        0.00419283, 0.00482168, 0.00417333, 0.00460587, 0.00384507,\n",
       "        0.00470033, 0.00510898, 0.00415688, 0.00423484, 0.00548759,\n",
       "        0.00609093, 0.00676136, 0.01571498, 0.00443702, 0.00526209,\n",
       "        0.00437121, 0.0051044 , 0.00538621, 0.00650811, 0.00504451,\n",
       "        0.00419621, 0.00515056, 0.00408983, 0.004954  , 0.00419688,\n",
       "        0.00420756, 0.00552969, 0.0071969 , 0.00523362, 0.0049304 ,\n",
       "        0.00474138, 0.00612359, 0.00449996, 0.00455136, 0.00524826,\n",
       "        0.00467238, 0.00404491, 0.00455947, 0.0052928 , 0.00620494,\n",
       "        0.00571752, 0.00662379, 0.0046422 , 0.0050765 , 0.00490575,\n",
       "        0.0209784 , 0.0170701 , 0.00763936, 0.00610394, 0.00692878,\n",
       "        0.0126482 , 0.0064919 , 0.00502825, 0.00656505, 0.01759357,\n",
       "        0.01100435, 0.00415277, 0.00901041, 0.00762458, 0.00935154,\n",
       "        0.0066206 , 0.00609455, 0.01661582, 0.00745716, 0.00501237,\n",
       "        0.00531898, 0.00546942, 0.00451202, 0.00392609, 0.00433497,\n",
       "        0.0039525 , 0.00501895, 0.00683174, 0.00422106, 0.00595126,\n",
       "        0.00518098, 0.00460491, 0.00518885, 0.00546403, 0.0043685 ,\n",
       "        0.00618496, 0.0066741 , 0.00521007, 0.00439487, 0.00564961,\n",
       "        0.00436363, 0.00515189, 0.00623989, 0.0049952 , 0.00465961,\n",
       "        0.00549922, 0.00531154, 0.00445266, 0.00379782, 0.00339789,\n",
       "        0.00434065, 0.0058672 , 0.00438156, 0.00572162, 0.00439854,\n",
       "        0.00496478, 0.0049109 , 0.0059412 , 0.00629768, 0.00387526,\n",
       "        0.00403395, 0.0045042 , 0.00485282, 0.00513949, 0.00488191,\n",
       "        0.00472074, 0.00479655, 0.00462227, 0.00523062, 0.00534573,\n",
       "        0.00607619, 0.00424819, 0.00432186, 0.00479331, 0.00508394,\n",
       "        0.00448213, 0.00457559, 0.00631065, 0.00474806, 0.00462384,\n",
       "        0.00514803, 0.00494328, 0.00427732, 0.00632362, 0.0043293 ,\n",
       "        0.00410256, 0.00426908, 0.00432467, 0.00483084, 0.00481615,\n",
       "        0.00424299, 0.00426345, 0.00482273, 0.00554776, 0.00447111,\n",
       "        0.00392489, 0.00398717, 0.00401521, 0.00471644, 0.00425906,\n",
       "        0.00472593, 0.00545654, 0.00496898, 0.0049068 , 0.00503802,\n",
       "        0.00431027, 0.00363607]),\n",
       " 'std_score_time': array([1.44804583e-02, 4.16303090e-03, 2.62771205e-03, 2.31994270e-03,\n",
       "        1.00694236e-02, 2.30853233e-03, 2.89776236e-04, 2.96237062e-03,\n",
       "        1.52280928e-03, 2.77827686e-03, 2.48340072e-03, 2.53826529e-03,\n",
       "        3.34183786e-03, 1.20604959e-03, 1.74008861e-03, 2.14408217e-03,\n",
       "        4.67944106e-04, 1.50808458e-03, 1.50385821e-03, 8.08088258e-04,\n",
       "        1.61614748e-03, 9.52746457e-04, 4.45413143e-04, 4.20730682e-03,\n",
       "        2.31474008e-03, 5.26421649e-04, 2.01048513e-04, 5.72843203e-04,\n",
       "        1.89864318e-04, 5.54852069e-04, 3.35951827e-03, 8.26876517e-04,\n",
       "        5.18716493e-04, 4.31345112e-04, 2.15816277e-04, 9.69400589e-04,\n",
       "        1.40898191e-04, 1.73517614e-03, 2.08072787e-03, 2.00011129e-04,\n",
       "        3.69406857e-04, 1.15852327e-03, 1.80956680e-04, 1.12999790e-03,\n",
       "        2.18041558e-04, 1.54297475e-03, 2.65520728e-03, 3.68682258e-04,\n",
       "        3.72271854e-04, 2.47036564e-03, 3.19956488e-03, 3.46122549e-03,\n",
       "        2.19015817e-02, 9.75274509e-04, 9.95940373e-04, 3.82026516e-04,\n",
       "        1.16241866e-03, 1.16246257e-03, 2.07233579e-03, 1.78858256e-03,\n",
       "        6.26029050e-04, 1.79099207e-03, 1.97953248e-04, 9.66924301e-04,\n",
       "        4.58852868e-04, 2.87454604e-04, 2.34611359e-03, 3.50291310e-03,\n",
       "        1.43202430e-03, 4.77662174e-04, 2.90214369e-04, 2.73546171e-03,\n",
       "        1.14760254e-03, 1.34995108e-03, 2.15722503e-03, 8.93975604e-04,\n",
       "        4.97415899e-05, 6.49919345e-04, 1.10007227e-03, 2.10555880e-03,\n",
       "        1.90312346e-03, 3.64962693e-03, 1.00613098e-03, 1.45384272e-03,\n",
       "        1.86792780e-03, 2.23287283e-02, 2.00661043e-02, 6.61984604e-03,\n",
       "        3.27726699e-03, 5.81116380e-03, 1.45436584e-02, 2.42516457e-03,\n",
       "        2.37705056e-03, 2.93927170e-03, 1.18881841e-02, 7.81880111e-03,\n",
       "        2.12740245e-04, 6.46892817e-03, 6.56515003e-03, 9.67549845e-03,\n",
       "        1.67741201e-03, 4.00151753e-03, 1.26692906e-02, 4.88634356e-03,\n",
       "        1.42672599e-03, 1.09408409e-03, 1.69238698e-03, 5.23356976e-04,\n",
       "        5.53572216e-04, 5.75749333e-04, 4.77897062e-04, 1.13927281e-03,\n",
       "        3.64587568e-03, 5.65482631e-05, 1.52016737e-03, 1.43548686e-03,\n",
       "        7.07034927e-04, 7.23994509e-04, 1.14410795e-03, 2.38896673e-04,\n",
       "        4.43279936e-03, 3.44433785e-03, 3.36420232e-03, 3.80635709e-04,\n",
       "        2.73122806e-03, 1.49021309e-04, 3.21637978e-04, 3.24006363e-03,\n",
       "        1.57415428e-03, 4.14794611e-05, 1.65933217e-03, 1.41168582e-03,\n",
       "        8.85863863e-04, 6.37930946e-04, 5.31957948e-04, 4.58969079e-05,\n",
       "        3.21269961e-03, 3.58470230e-04, 1.60088909e-03, 8.70307370e-05,\n",
       "        1.47066487e-03, 4.26380348e-04, 1.27869034e-03, 2.03155473e-03,\n",
       "        2.33781009e-04, 5.86121256e-04, 9.17665845e-04, 9.00787425e-04,\n",
       "        2.02748896e-03, 6.97047163e-04, 8.58149779e-05, 5.85641808e-04,\n",
       "        3.24945243e-04, 6.12936607e-04, 1.28800861e-03, 2.79509262e-03,\n",
       "        1.00506326e-03, 6.78210594e-04, 1.64571956e-03, 6.63743118e-04,\n",
       "        6.05722520e-04, 3.88824217e-04, 2.83104936e-03, 2.37713458e-04,\n",
       "        2.88657004e-04, 9.57904560e-04, 6.88018446e-04, 1.18872032e-04,\n",
       "        3.25387425e-03, 1.31337452e-03, 6.44268175e-04, 9.43995956e-05,\n",
       "        1.63010463e-04, 1.36806057e-03, 2.01686651e-04, 6.47559257e-05,\n",
       "        1.51192980e-04, 8.73896755e-04, 1.87599593e-03, 5.06186831e-04,\n",
       "        2.57610073e-04, 1.74603456e-04, 6.33216101e-04, 7.66917651e-04,\n",
       "        1.89544024e-04, 9.47530087e-04, 1.30965827e-03, 7.51605298e-04,\n",
       "        5.47308938e-04, 3.75721387e-04, 5.88967064e-04, 5.82312483e-04]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
       "                    0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
       "                    0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4,\n",
       "                    6, 6, 6, 8, 8, 8, 2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8,\n",
       "                    2, 2, 2, 4, 4, 4, 6, 6, 6, 8, 8, 8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5,\n",
       "                    1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.2,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.3,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.2,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 5},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 1},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 3},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.3,\n",
       "   'max_depth': 8,\n",
       "   'min_child_weight': 5}],\n",
       " 'split0_test_score': array([0.88865595, 0.88259958, 0.88108549, 0.8914512 , 0.88353133,\n",
       "        0.8795714 , 0.8858607 , 0.87794083, 0.87561146, 0.88306546,\n",
       "        0.87700908, 0.8758444 , 0.89587701, 0.89389704, 0.89447939,\n",
       "        0.89983694, 0.89121826, 0.89098532, 0.89657582, 0.88842301,\n",
       "        0.89028651, 0.89121826, 0.88655952, 0.88865595, 0.90170044,\n",
       "        0.90100163, 0.88935476, 0.89843932, 0.89890519, 0.88935476,\n",
       "        0.89890519, 0.89284882, 0.89075239, 0.89098532, 0.89354764,\n",
       "        0.8914512 , 0.89471232, 0.89797345, 0.90006988, 0.89634288,\n",
       "        0.89913813, 0.89378057, 0.89308176, 0.89308176, 0.89657582,\n",
       "        0.88539483, 0.8951782 , 0.89494526, 0.90612625, 0.89855579,\n",
       "        0.89762404, 0.90775681, 0.90263219, 0.90053576, 0.91125087,\n",
       "        0.90170044, 0.90100163, 0.91148381, 0.90309807, 0.90100163,\n",
       "        0.91101794, 0.90740741, 0.89983694, 0.90845563, 0.91194969,\n",
       "        0.9089215 , 0.91660843, 0.91101794, 0.90868856, 0.90798975,\n",
       "        0.90822269, 0.90822269, 0.910785  , 0.90717447, 0.90729094,\n",
       "        0.90542744, 0.90985325, 0.91194969, 0.91497787, 0.91614256,\n",
       "        0.91311437, 0.90542744, 0.91823899, 0.91171675, 0.91730724,\n",
       "        0.90635919, 0.91206615, 0.90682506, 0.91404612, 0.91684137,\n",
       "        0.90053576, 0.90868856, 0.91590962, 0.91008619, 0.90985325,\n",
       "        0.91777312, 0.91171675, 0.91206615, 0.90612625, 0.91311437,\n",
       "        0.91311437, 0.91194969, 0.91404612, 0.91031912, 0.90659213,\n",
       "        0.91590962, 0.91218262, 0.90915444, 0.91649196, 0.91043559,\n",
       "        0.90752388, 0.92173305, 0.91893781, 0.91590962, 0.91358025,\n",
       "        0.91288143, 0.91660843, 0.91218262, 0.91870487, 0.91311437,\n",
       "        0.91660843, 0.91940368, 0.91602609, 0.91986956, 0.92056837,\n",
       "        0.91893781, 0.91125087, 0.91614256, 0.92033543, 0.9126485 ,\n",
       "        0.91311437, 0.91986956, 0.92394596, 0.92068484, 0.91998602,\n",
       "        0.91637549, 0.91474493, 0.92685768, 0.903331  , 0.91148381,\n",
       "        0.92546005, 0.90472863, 0.91684137, 0.92056837, 0.90915444,\n",
       "        0.91043559, 0.90531097, 0.91381318, 0.91847193, 0.91754018,\n",
       "        0.92126718, 0.92243187, 0.91940368, 0.92056837, 0.92243187,\n",
       "        0.91986956, 0.91497787, 0.91055206, 0.91241556, 0.91917074,\n",
       "        0.91963662, 0.91684137, 0.91940368, 0.92382949, 0.91963662,\n",
       "        0.91358025, 0.92872117, 0.91823899, 0.91637549, 0.91521081,\n",
       "        0.91986956, 0.91404612, 0.92173305, 0.92150012, 0.90775681,\n",
       "        0.91777312, 0.92499418, 0.90659213, 0.92499418, 0.9282553 ,\n",
       "        0.91695784, 0.91695784, 0.91975309, 0.90496157, 0.92126718,\n",
       "        0.93035174, 0.903331  , 0.91474493, 0.92732355, 0.90845563,\n",
       "        0.92173305, 0.93058467]),\n",
       " 'split1_test_score': array([0.79058933, 0.78907524, 0.78418355, 0.7869788 , 0.7851153 ,\n",
       "        0.77113906, 0.78977405, 0.78441649, 0.76927556, 0.79140461,\n",
       "        0.78045656, 0.76811088, 0.81038901, 0.80433263, 0.79920801,\n",
       "        0.80654554, 0.80142092, 0.78895877, 0.80421617, 0.79676217,\n",
       "        0.78942464, 0.80607966, 0.80095504, 0.78919171, 0.82320056,\n",
       "        0.82215234, 0.81795947, 0.82168647, 0.82424878, 0.81714419,\n",
       "        0.82168647, 0.8256464 , 0.81621244, 0.81586303, 0.8237829 ,\n",
       "        0.8159795 , 0.83484743, 0.82797577, 0.82250175, 0.8312369 ,\n",
       "        0.83403215, 0.81854181, 0.82098765, 0.83752621, 0.82273468,\n",
       "        0.81889122, 0.83915677, 0.82320056, 0.81318425, 0.81388307,\n",
       "        0.80305148, 0.81842534, 0.81667831, 0.80479851, 0.81912416,\n",
       "        0.8200559 , 0.80899138, 0.82052178, 0.8219194 , 0.80689495,\n",
       "        0.82518053, 0.82587934, 0.82180294, 0.83473096, 0.83170277,\n",
       "        0.81924062, 0.82983927, 0.8331004 , 0.82692756, 0.82494759,\n",
       "        0.83426508, 0.82552993, 0.8197065 , 0.82063825, 0.82226881,\n",
       "        0.82727696, 0.84241789, 0.83018868, 0.82844165, 0.84334964,\n",
       "        0.83181924, 0.83263452, 0.84544608, 0.82506406, 0.83275099,\n",
       "        0.82552993, 0.82273468, 0.82867459, 0.84183555, 0.83414861,\n",
       "        0.8293734 , 0.84253436, 0.83368274, 0.82960634, 0.84253436,\n",
       "        0.83391568, 0.8237829 , 0.82471465, 0.81365013, 0.83077102,\n",
       "        0.82820871, 0.81772653, 0.83752621, 0.82890752, 0.82052178,\n",
       "        0.83519683, 0.83193571, 0.82098765, 0.82669462, 0.8331004 ,\n",
       "        0.82716049, 0.83193571, 0.83542977, 0.82809224, 0.83496389,\n",
       "        0.84195201, 0.8315863 , 0.83845795, 0.84591195, 0.83554624,\n",
       "        0.83542977, 0.83146983, 0.82692756, 0.83263452, 0.83251805,\n",
       "        0.83321686, 0.8393897 , 0.84300023, 0.82552993, 0.83659446,\n",
       "        0.85301654, 0.8390403 , 0.83007221, 0.83263452, 0.82785931,\n",
       "        0.83962264, 0.84113673, 0.83764267, 0.83985558, 0.85546238,\n",
       "        0.81993944, 0.8412532 , 0.86105288, 0.8297228 , 0.82704403,\n",
       "        0.82820871, 0.81772653, 0.83542977, 0.8256464 , 0.82122059,\n",
       "        0.83263452, 0.82844165, 0.82494759, 0.83542977, 0.83403215,\n",
       "        0.82448171, 0.82995574, 0.8315863 , 0.82646168, 0.82960634,\n",
       "        0.8293734 , 0.83624505, 0.83193571, 0.83379921, 0.83205218,\n",
       "        0.84195201, 0.84078733, 0.8334498 , 0.83077102, 0.82855812,\n",
       "        0.82739343, 0.83170277, 0.83135337, 0.83740974, 0.82867459,\n",
       "        0.83810855, 0.83880736, 0.83682739, 0.84416492, 0.8409038 ,\n",
       "        0.84195201, 0.83170277, 0.83205218, 0.83496389, 0.84777545,\n",
       "        0.83927324, 0.8431167 , 0.84696017, 0.83927324, 0.8505707 ,\n",
       "        0.85126951, 0.84323317]),\n",
       " 'split2_test_score': array([0.82308409, 0.82506406, 0.82250175, 0.81341719, 0.81854181,\n",
       "        0.81528069, 0.81458188, 0.81434894, 0.81201957, 0.80759376,\n",
       "        0.81807594, 0.81178663, 0.82541346, 0.83228512, 0.83007221,\n",
       "        0.82401584, 0.82273468, 0.82483112, 0.81551363, 0.82063825,\n",
       "        0.82180294, 0.80642907, 0.82273468, 0.82320056, 0.81842534,\n",
       "        0.82785931, 0.83065455, 0.81586303, 0.82587934, 0.82366643,\n",
       "        0.81644538, 0.82762637, 0.82389937, 0.8159795 , 0.82646168,\n",
       "        0.82320056, 0.82762637, 0.82762637, 0.82879106, 0.82250175,\n",
       "        0.82948987, 0.82110412, 0.82366643, 0.83321686, 0.82320056,\n",
       "        0.814116  , 0.82785931, 0.82296762, 0.83426508, 0.83927324,\n",
       "        0.83601211, 0.82098765, 0.82483112, 0.82622874, 0.82599581,\n",
       "        0.82250175, 0.82226881, 0.82739343, 0.82832518, 0.82366643,\n",
       "        0.83473096, 0.8331004 , 0.83624505, 0.82168647, 0.825297  ,\n",
       "        0.82692756, 0.82541346, 0.82599581, 0.8315863 , 0.82622874,\n",
       "        0.8234335 , 0.83181924, 0.83810855, 0.83170277, 0.83426508,\n",
       "        0.83181924, 0.83810855, 0.8334498 , 0.8371768 , 0.83554624,\n",
       "        0.8334498 , 0.84020498, 0.83554624, 0.83787561, 0.82879106,\n",
       "        0.83647799, 0.83193571, 0.83391568, 0.83181924, 0.8334498 ,\n",
       "        0.82832518, 0.83857442, 0.8393897 , 0.83997205, 0.84183555,\n",
       "        0.83542977, 0.84195201, 0.84346611, 0.84230142, 0.83240158,\n",
       "        0.82785931, 0.83042162, 0.8331004 , 0.82809224, 0.8297228 ,\n",
       "        0.83508036, 0.82948987, 0.8297228 , 0.83834149, 0.84393198,\n",
       "        0.83869089, 0.8297228 , 0.82995574, 0.83461449, 0.84183555,\n",
       "        0.83694386, 0.83228512, 0.84067086, 0.8353133 , 0.83112043,\n",
       "        0.83764267, 0.82576287, 0.83181924, 0.83391568, 0.83508036,\n",
       "        0.82879106, 0.83228512, 0.83880736, 0.83647799, 0.84532961,\n",
       "        0.83892383, 0.83065455, 0.83566271, 0.83438155, 0.83659446,\n",
       "        0.82506406, 0.8315863 , 0.83857442, 0.83508036, 0.83414861,\n",
       "        0.8334498 , 0.84183555, 0.83275099, 0.83414861, 0.84544608,\n",
       "        0.8487072 , 0.84463079, 0.83484743, 0.8353133 , 0.82925693,\n",
       "        0.83438155, 0.83414861, 0.83484743, 0.83601211, 0.8353133 ,\n",
       "        0.83414861, 0.84323317, 0.83892383, 0.83508036, 0.83973911,\n",
       "        0.83787561, 0.83403215, 0.84253436, 0.84113673, 0.83985558,\n",
       "        0.84649429, 0.83577918, 0.8393897 , 0.83030515, 0.82914046,\n",
       "        0.82890752, 0.83764267, 0.83088749, 0.83508036, 0.84812485,\n",
       "        0.83275099, 0.82692756, 0.83205218, 0.83181924, 0.83508036,\n",
       "        0.82960634, 0.83030515, 0.83240158, 0.84439786, 0.82273468,\n",
       "        0.84043792, 0.84765898, 0.8234335 , 0.84754251, 0.8353133 ,\n",
       "        0.82995574, 0.83869089]),\n",
       " 'split3_test_score': array([0.92079327, 0.91598558, 0.9140625 , 0.91694712, 0.90192308,\n",
       "        0.90204327, 0.915625  , 0.90360577, 0.90252404, 0.91322115,\n",
       "        0.90673077, 0.89987981, 0.92163462, 0.91346154, 0.90721154,\n",
       "        0.91274038, 0.89591346, 0.89867788, 0.91346154, 0.89302885,\n",
       "        0.8984375 , 0.90625   , 0.89615385, 0.89627404, 0.91286058,\n",
       "        0.91009615, 0.90721154, 0.90144231, 0.89543269, 0.89362981,\n",
       "        0.89350962, 0.88653846, 0.89651442, 0.88894231, 0.88605769,\n",
       "        0.89939904, 0.90564904, 0.90012019, 0.89951923, 0.89086538,\n",
       "        0.88653846, 0.89483173, 0.88197115, 0.88485577, 0.89170673,\n",
       "        0.88125   , 0.87716346, 0.89507212, 0.92475962, 0.91682692,\n",
       "        0.91742788, 0.91802885, 0.90721154, 0.91382212, 0.91826923,\n",
       "        0.90432692, 0.91045673, 0.91610577, 0.90480769, 0.90973558,\n",
       "        0.91117788, 0.90949519, 0.91057692, 0.91081731, 0.90432692,\n",
       "        0.90492788, 0.90576923, 0.896875  , 0.91021635, 0.90336538,\n",
       "        0.89735577, 0.91045673, 0.90192308, 0.90552885, 0.90324519,\n",
       "        0.89375   , 0.89326923, 0.90612981, 0.89278846, 0.90072115,\n",
       "        0.90733173, 0.89182692, 0.88774038, 0.90853365, 0.89783654,\n",
       "        0.90072115, 0.89579327, 0.89615385, 0.88966346, 0.89603365,\n",
       "        0.88509615, 0.89783654, 0.90396635, 0.88509615, 0.89783654,\n",
       "        0.89867788, 0.92427885, 0.91983173, 0.91887019, 0.9203125 ,\n",
       "        0.91370192, 0.9140625 , 0.92067308, 0.90841346, 0.91478365,\n",
       "        0.91706731, 0.90793269, 0.91502404, 0.91682692, 0.91105769,\n",
       "        0.90985577, 0.90793269, 0.90384615, 0.91382212, 0.90384615,\n",
       "        0.90793269, 0.91262019, 0.90360577, 0.90961538, 0.90877404,\n",
       "        0.90649038, 0.90360577, 0.90252404, 0.89783654, 0.89567308,\n",
       "        0.90276442, 0.88173077, 0.90072115, 0.90637019, 0.89158654,\n",
       "        0.89447115, 0.90949519, 0.90745192, 0.90480769, 0.90036058,\n",
       "        0.89615385, 0.89326923, 0.89387019, 0.87572115, 0.89471154,\n",
       "        0.89435096, 0.89326923, 0.88533654, 0.89603365, 0.92007212,\n",
       "        0.91670673, 0.91694712, 0.91610577, 0.91165865, 0.91454327,\n",
       "        0.90913462, 0.90889423, 0.91213942, 0.91225962, 0.90745192,\n",
       "        0.91153846, 0.91322115, 0.90925481, 0.90877404, 0.89831731,\n",
       "        0.89639423, 0.90949519, 0.90288462, 0.90504808, 0.91189904,\n",
       "        0.89879808, 0.90697115, 0.91454327, 0.90456731, 0.89879808,\n",
       "        0.9015625 , 0.88774038, 0.89615385, 0.90204327, 0.89399038,\n",
       "        0.89447115, 0.9046875 , 0.89254808, 0.90697115, 0.90300481,\n",
       "        0.89447115, 0.89459135, 0.89675481, 0.89399038, 0.89723558,\n",
       "        0.89939904, 0.884375  , 0.89591346, 0.89507212, 0.88725962,\n",
       "        0.89975962, 0.89627404]),\n",
       " 'split4_test_score': array([0.87620192, 0.87055288, 0.86706731, 0.87680288, 0.87331731,\n",
       "        0.86875   , 0.86947115, 0.87475962, 0.86730769, 0.86586538,\n",
       "        0.87355769, 0.86802885, 0.87271635, 0.87331731, 0.87764423,\n",
       "        0.88088942, 0.87475962, 0.88100962, 0.87259615, 0.87956731,\n",
       "        0.88016827, 0.87259615, 0.87908654, 0.87896635, 0.87307692,\n",
       "        0.87608173, 0.88521635, 0.88365385, 0.86778846, 0.8828125 ,\n",
       "        0.87644231, 0.88569712, 0.88329327, 0.87091346, 0.8875    ,\n",
       "        0.88377404, 0.87704327, 0.88076923, 0.88786058, 0.87668269,\n",
       "        0.88125   , 0.8890625 , 0.86514423, 0.88028846, 0.88858173,\n",
       "        0.8625    , 0.89014423, 0.8921875 , 0.88185096, 0.87427885,\n",
       "        0.87487981, 0.88762019, 0.88149038, 0.88557692, 0.89014423,\n",
       "        0.88004808, 0.88269231, 0.89254808, 0.88317308, 0.88389423,\n",
       "        0.88557692, 0.87944712, 0.88353365, 0.89591346, 0.88605769,\n",
       "        0.8890625 , 0.88389423, 0.88293269, 0.89338942, 0.88581731,\n",
       "        0.88918269, 0.89242788, 0.89134615, 0.89290865, 0.8984375 ,\n",
       "        0.88341346, 0.88245192, 0.90012019, 0.88846154, 0.88173077,\n",
       "        0.89795673, 0.87716346, 0.87764423, 0.89771635, 0.88076923,\n",
       "        0.88521635, 0.89591346, 0.87764423, 0.88028846, 0.90084135,\n",
       "        0.87884615, 0.88653846, 0.90324519, 0.87379808, 0.88125   ,\n",
       "        0.90276442, 0.88581731, 0.88197115, 0.87824519, 0.89483173,\n",
       "        0.88629808, 0.88762019, 0.89879808, 0.88822115, 0.8890625 ,\n",
       "        0.896875  , 0.89278846, 0.88930288, 0.88954327, 0.87992788,\n",
       "        0.88762019, 0.88581731, 0.88293269, 0.89278846, 0.88990385,\n",
       "        0.88677885, 0.89447115, 0.89471154, 0.88870192, 0.89423077,\n",
       "        0.87367788, 0.87824519, 0.89663462, 0.87403846, 0.86730769,\n",
       "        0.90108173, 0.88052885, 0.88100962, 0.9046875 , 0.88557692,\n",
       "        0.87836538, 0.89747596, 0.86838942, 0.86730769, 0.89651442,\n",
       "        0.88245192, 0.89134615, 0.90024038, 0.88846154, 0.88894231,\n",
       "        0.91129808, 0.87548077, 0.88389423, 0.90480769, 0.88677885,\n",
       "        0.88209135, 0.88052885, 0.89194712, 0.89038462, 0.89423077,\n",
       "        0.89158654, 0.89375   , 0.89735577, 0.89879808, 0.89663462,\n",
       "        0.896875  , 0.88858173, 0.88185096, 0.89170673, 0.88629808,\n",
       "        0.88762019, 0.89242788, 0.89615385, 0.88677885, 0.89663462,\n",
       "        0.89399038, 0.89375   , 0.89507212, 0.87391827, 0.88798077,\n",
       "        0.88461538, 0.88293269, 0.88317308, 0.89375   , 0.88461538,\n",
       "        0.88413462, 0.89819712, 0.88389423, 0.88533654, 0.90300481,\n",
       "        0.87103365, 0.87199519, 0.8921875 , 0.88581731, 0.88269231,\n",
       "        0.90552885, 0.87644231, 0.89338942, 0.915625  , 0.87307692,\n",
       "        0.89483173, 0.90649038]),\n",
       " 'mean_test_score': array([0.85986491, 0.85665547, 0.85378012, 0.85711944, 0.85248577,\n",
       "        0.84735688, 0.85506256, 0.85101433, 0.84534766, 0.85223007,\n",
       "        0.85116601, 0.84473011, 0.86520609, 0.86345873, 0.86172308,\n",
       "        0.86480563, 0.85720939, 0.85689254, 0.86047266, 0.85568392,\n",
       "        0.85602397, 0.85651463, 0.85709793, 0.85525772, 0.86585277,\n",
       "        0.86743823, 0.86607933, 0.86421699, 0.86245089, 0.86132154,\n",
       "        0.86139779, 0.86367143, 0.86213438, 0.85653673, 0.86346998,\n",
       "        0.86276087, 0.86797568, 0.866893  , 0.8677485 , 0.86352592,\n",
       "        0.86608972, 0.86346415, 0.85697025, 0.86579381, 0.86455991,\n",
       "        0.85243041, 0.86590039, 0.86567461, 0.87203723, 0.86856357,\n",
       "        0.86579906, 0.87056377, 0.86656871, 0.86619241, 0.87295686,\n",
       "        0.86572662, 0.86508217, 0.87361057, 0.86826468, 0.86503856,\n",
       "        0.87353685, 0.87106589, 0.8703991 , 0.87432076, 0.87186681,\n",
       "        0.86981601, 0.87230493, 0.86998437, 0.87416164, 0.86966976,\n",
       "        0.87049195, 0.8736913 , 0.87237386, 0.8715906 , 0.8731015 ,\n",
       "        0.86833742, 0.87322017, 0.87636763, 0.87236926, 0.87549807,\n",
       "        0.87673438, 0.86945147, 0.87292318, 0.87618128, 0.87149101,\n",
       "        0.87086092, 0.87168866, 0.86864268, 0.87153057, 0.87626296,\n",
       "        0.86443533, 0.87483447, 0.87923872, 0.86771176, 0.87466194,\n",
       "        0.87771217, 0.87750956, 0.87640996, 0.87183864, 0.87828624,\n",
       "        0.87383648, 0.87235611, 0.88082878, 0.8727907 , 0.87213657,\n",
       "        0.88002582, 0.87486587, 0.87283836, 0.87757965, 0.87569071,\n",
       "        0.87417024, 0.87542831, 0.87422043, 0.87704539, 0.87682594,\n",
       "        0.87729777, 0.87751424, 0.87792575, 0.87964949, 0.87655717,\n",
       "        0.87396983, 0.87169747, 0.87478631, 0.87165895, 0.87022951,\n",
       "        0.87695838, 0.86903706, 0.87593618, 0.87868021, 0.8743472 ,\n",
       "        0.87557826, 0.87930711, 0.87310444, 0.87196326, 0.87626296,\n",
       "        0.87193359, 0.87441667, 0.87943707, 0.86848993, 0.87694973,\n",
       "        0.87689967, 0.87131348, 0.8759752 , 0.87705623, 0.8776991 ,\n",
       "        0.87722992, 0.87302885, 0.87842865, 0.87629498, 0.87535835,\n",
       "        0.87780088, 0.87753327, 0.87773878, 0.88061359, 0.87917277,\n",
       "        0.87738267, 0.87799393, 0.87443359, 0.87488767, 0.87462631,\n",
       "        0.87418001, 0.87780833, 0.87858244, 0.87811847, 0.88001561,\n",
       "        0.878963  , 0.88120177, 0.88013878, 0.87118745, 0.87193765,\n",
       "        0.87246968, 0.87081293, 0.87266017, 0.8779567 , 0.8726324 ,\n",
       "        0.87344769, 0.87872274, 0.8703828 , 0.87865721, 0.88204981,\n",
       "        0.8708042 , 0.86911046, 0.87462983, 0.8728262 , 0.87434104,\n",
       "        0.88299816, 0.8709848 , 0.8748883 , 0.88496728, 0.87093523,\n",
       "        0.87950993, 0.88305463]),\n",
       " 'std_test_score': array([0.04681885, 0.04461256, 0.04555901, 0.04893885, 0.04366417,\n",
       "        0.04759008, 0.04631461, 0.04434943, 0.04809367, 0.04593211,\n",
       "        0.04550049, 0.04797541, 0.04187996, 0.03997913, 0.04075677,\n",
       "        0.04205156, 0.03811424, 0.0427385 , 0.04346112, 0.0393367 ,\n",
       "        0.0428073 , 0.04240003, 0.0379729 , 0.0418384 , 0.03902704,\n",
       "        0.03643683, 0.03512909, 0.03763412, 0.03237927, 0.03364845,\n",
       "        0.03538932, 0.03034634, 0.03469702, 0.03389028, 0.03142289,\n",
       "        0.03566667, 0.03143851, 0.03261597, 0.03470878, 0.03073455,\n",
       "        0.02866154, 0.03569503, 0.02966409, 0.0252127 , 0.03405572,\n",
       "        0.03036936, 0.02732821, 0.03479037, 0.04225737, 0.0377079 ,\n",
       "        0.04143753, 0.04266939, 0.03848675, 0.04287218, 0.04223351,\n",
       "        0.03726502, 0.04156522, 0.04136012, 0.0360945 , 0.04180678,\n",
       "        0.03690765, 0.03563977, 0.03516091, 0.03821296, 0.03645154,\n",
       "        0.0388067 , 0.03799767, 0.03426351, 0.03716309, 0.03674712,\n",
       "        0.03470303, 0.03733085, 0.03648694, 0.03757567, 0.0369099 ,\n",
       "        0.03246011, 0.02832236, 0.03658002, 0.03364385, 0.03148554,\n",
       "        0.0363346 , 0.02851403, 0.02982411, 0.03702296, 0.03522322,\n",
       "        0.03345123, 0.03681034, 0.03193493, 0.03056749, 0.03535016,\n",
       "        0.02990339, 0.02887998, 0.03520174, 0.02951734, 0.02803086,\n",
       "        0.03571552, 0.03892919, 0.03727048, 0.0392097 , 0.03902834,\n",
       "        0.03868615, 0.04070125, 0.03786018, 0.03698418, 0.03938391,\n",
       "        0.03734527, 0.03663101, 0.039792  , 0.03827941, 0.03255314,\n",
       "        0.03474341, 0.03818166, 0.03582543, 0.03822981, 0.03233809,\n",
       "        0.03216317, 0.03795621, 0.03181318, 0.03349371, 0.0358695 ,\n",
       "        0.03370674, 0.03759028, 0.03764061, 0.03453303, 0.03419804,\n",
       "        0.03806198, 0.02934235, 0.0307243 , 0.03945694, 0.02883602,\n",
       "        0.02693046, 0.0370816 , 0.03752465, 0.0358702 , 0.03692974,\n",
       "        0.03438921, 0.03228123, 0.03551432, 0.02683722, 0.0280911 ,\n",
       "        0.04237515, 0.02603291, 0.02796926, 0.03769644, 0.03598207,\n",
       "        0.03435489, 0.0371268 , 0.03633898, 0.03865949, 0.04177839,\n",
       "        0.03738022, 0.03887053, 0.03982656, 0.03730791, 0.03724864,\n",
       "        0.04004847, 0.03531812, 0.03367168, 0.03679458, 0.03442561,\n",
       "        0.03483118, 0.03573577, 0.03475947, 0.03527478, 0.03681274,\n",
       "        0.02912606, 0.03681345, 0.03660232, 0.03596799, 0.03623483,\n",
       "        0.03786869, 0.0314083 , 0.03611601, 0.03523651, 0.02955127,\n",
       "        0.03294242, 0.03865345, 0.03026607, 0.03571252, 0.03718143,\n",
       "        0.03231114, 0.03421157, 0.03586077, 0.02789659, 0.03511385,\n",
       "        0.03672118, 0.02269839, 0.03406017, 0.03556576, 0.02593256,\n",
       "        0.03370679, 0.03615794]),\n",
       " 'rank_test_score': array([171, 177, 184, 173, 185, 190, 183, 189, 191, 187, 188, 192, 152,\n",
       "        163, 167, 155, 172, 176, 170, 181, 180, 179, 174, 182, 147, 140,\n",
       "        145, 158, 165, 169, 168, 159, 166, 178, 161, 164, 137, 141, 138,\n",
       "        160, 144, 162, 175, 149, 156, 186, 146, 151, 101, 133, 148, 121,\n",
       "        142, 143,  88, 150, 153,  81, 136, 154,  82, 115, 123,  73, 105,\n",
       "        127,  99, 126,  77, 128, 122,  80,  96, 110,  86, 135,  84,  49,\n",
       "         97,  58,  46, 129,  89,  53, 112, 118, 108, 132, 111,  51, 157,\n",
       "         64,  15, 139,  66,  31,  36,  48, 106,  23,  79,  98,   6,  92,\n",
       "        100,   9,  63,  90,  33,  56,  76,  59,  74,  41,  45,  38,  35,\n",
       "         27,  11,  47,  78, 107,  65, 109, 125,  42, 131,  55,  19,  71,\n",
       "         57,  14,  85, 102,  51, 104,  70,  13, 134,  43,  44, 113,  54,\n",
       "         40,  32,  39,  87,  22,  50,  60,  29,  34,  30,   7,  16,  37,\n",
       "         25,  69,  62,  68,  75,  28,  21,  24,  10,  17,   5,   8, 114,\n",
       "        103,  95, 119,  93,  26,  94,  83,  18, 124,  20,   4, 120, 130,\n",
       "         67,  91,  72,   3, 116,  61,   1, 117,  12,   2], dtype=int32),\n",
       " 'split0_train_score': array([0.8755176 , 0.87169476, 0.87033422, 0.89887607, 0.89273883,\n",
       "        0.88327418, 0.9153357 , 0.9030908 , 0.88755546, 0.92392044,\n",
       "        0.90543478, 0.88918219, 0.89031352, 0.88831707, 0.88361432,\n",
       "        0.91329488, 0.90786749, 0.89679828, 0.929126  , 0.91495859,\n",
       "        0.90199645, 0.93907867, 0.91696983, 0.90326087, 0.90672878,\n",
       "        0.90271369, 0.8947131 , 0.93349601, 0.91985359, 0.9119787 ,\n",
       "        0.94654688, 0.92829784, 0.91545401, 0.95190033, 0.92973233,\n",
       "        0.9152026 , 0.91469979, 0.91080302, 0.90521295, 0.94105294,\n",
       "        0.92829045, 0.91876664, 0.95271369, 0.9347974 , 0.92318841,\n",
       "        0.95743123, 0.9365942 , 0.92377995, 0.89487578, 0.89018781,\n",
       "        0.885315  , 0.9226856 , 0.91244454, 0.90404466, 0.94242088,\n",
       "        0.92477817, 0.91173469, 0.95156019, 0.92914079, 0.91362762,\n",
       "        0.90671399, 0.90208518, 0.89611062, 0.9399512 , 0.92920734,\n",
       "        0.91983881, 0.96188258, 0.94179976, 0.92454156, 0.96840432,\n",
       "        0.94478705, 0.92436409, 0.92610175, 0.9183008 , 0.91041852,\n",
       "        0.96518042, 0.94691659, 0.93331854, 0.97933304, 0.95735729,\n",
       "        0.93856847, 0.98541112, 0.9597974 , 0.93939663, 0.93984028,\n",
       "        0.92729222, 0.92091837, 0.97579858, 0.96001183, 0.94328601,\n",
       "        0.98754067, 0.96530612, 0.94905353, 0.98999556, 0.96719166,\n",
       "        0.94926057, 0.89834369, 0.89402544, 0.88991423, 0.93243123,\n",
       "        0.92366164, 0.91242236, 0.95661786, 0.93575126, 0.91964655,\n",
       "        0.96725081, 0.93988465, 0.920105  , 0.91538007, 0.90901361,\n",
       "        0.90187814, 0.95754215, 0.9386572 , 0.92700385, 0.97775067,\n",
       "        0.95340875, 0.93410234, 0.98533718, 0.95735729, 0.93433895,\n",
       "        0.93617273, 0.92519225, 0.91859657, 0.97797249, 0.96194173,\n",
       "        0.94307897, 0.99005472, 0.9717835 , 0.95035492, 0.9940772 ,\n",
       "        0.97389825, 0.94912748, 0.9444543 , 0.93562555, 0.92702603,\n",
       "        0.98690476, 0.96885537, 0.95298728, 0.99434339, 0.97826087,\n",
       "        0.95672878, 0.99676871, 0.98040521, 0.95782313, 0.90164153,\n",
       "        0.89760426, 0.8929163 , 0.93925614, 0.92625702, 0.91625259,\n",
       "        0.96550577, 0.94232476, 0.92434191, 0.9761535 , 0.94527507,\n",
       "        0.92471162, 0.91994232, 0.91311003, 0.90672138, 0.96346495,\n",
       "        0.94719757, 0.93368086, 0.98252736, 0.96139456, 0.94094203,\n",
       "        0.99030612, 0.96523957, 0.94240609, 0.94123041, 0.93053091,\n",
       "        0.922937  , 0.98289707, 0.96515824, 0.94926797, 0.99437297,\n",
       "        0.97537711, 0.95428128, 0.99696096, 0.97626442, 0.9560559 ,\n",
       "        0.95195209, 0.94117865, 0.93059746, 0.98989204, 0.97463768,\n",
       "        0.96155723, 0.99715321, 0.98096717, 0.96319876, 0.99827714,\n",
       "        0.98413191, 0.96167554]),\n",
       " 'split1_train_score': array([0.89188849, 0.89063886, 0.88623188, 0.91114315, 0.90557527,\n",
       "        0.8960071 , 0.92234546, 0.91231145, 0.90113872, 0.92972493,\n",
       "        0.91622301, 0.90278024, 0.90770482, 0.90602632, 0.89991127,\n",
       "        0.92661934, 0.91805679, 0.90878438, 0.93938184, 0.92620526,\n",
       "        0.91390121, 0.9476782 , 0.9300281 , 0.91608252, 0.92263384,\n",
       "        0.91976486, 0.91209701, 0.94547471, 0.93422064, 0.92395001,\n",
       "        0.95226264, 0.94114167, 0.92917036, 0.95853298, 0.94270186,\n",
       "        0.92925909, 0.93386572, 0.92467465, 0.91986838, 0.95371192,\n",
       "        0.94121562, 0.93269003, 0.96067731, 0.94593316, 0.93451642,\n",
       "        0.96437445, 0.94741201, 0.93437592, 0.91270334, 0.91020408,\n",
       "        0.90101301, 0.93768116, 0.92839397, 0.91904022, 0.95185596,\n",
       "        0.93774771, 0.92491127, 0.95976043, 0.94094203, 0.92625702,\n",
       "        0.92661195, 0.92380213, 0.91629695, 0.95419255, 0.94315291,\n",
       "        0.93418367, 0.96770186, 0.95244011, 0.93902692, 0.97627921,\n",
       "        0.95402248, 0.93969979, 0.94223602, 0.93649068, 0.93090062,\n",
       "        0.97073351, 0.95809672, 0.94838805, 0.98302277, 0.96498817,\n",
       "        0.95192251, 0.98753327, 0.96620822, 0.95256581, 0.95365277,\n",
       "        0.94184413, 0.93885685, 0.97962881, 0.96406389, 0.95479148,\n",
       "        0.9893079 , 0.97097752, 0.95647737, 0.99241349, 0.9726782 ,\n",
       "        0.95699497, 0.91930642, 0.91713251, 0.91206744, 0.94841763,\n",
       "        0.93768116, 0.92778764, 0.96356108, 0.94712363, 0.93432416,\n",
       "        0.9715173 , 0.95119048, 0.93494528, 0.93284531, 0.92907424,\n",
       "        0.92312186, 0.96605294, 0.95258799, 0.9425244 , 0.97966578,\n",
       "        0.96018929, 0.94655427, 0.98532978, 0.96351671, 0.94747856,\n",
       "        0.95224786, 0.94247264, 0.93754067, 0.98099675, 0.96453712,\n",
       "        0.95387459, 0.99222124, 0.9730627 , 0.95901361, 0.99522331,\n",
       "        0.97498521, 0.95787489, 0.95739426, 0.94880952, 0.94380361,\n",
       "        0.98606921, 0.97378734, 0.96016711, 0.99504584, 0.98135167,\n",
       "        0.96183821, 0.9970423 , 0.98154392, 0.9634058 , 0.92253032,\n",
       "        0.9211328 , 0.91523957, 0.95675096, 0.94321207, 0.93305235,\n",
       "        0.97228631, 0.95246968, 0.9387829 , 0.97971014, 0.95573795,\n",
       "        0.93977374, 0.93896776, 0.93192842, 0.92659716, 0.97307749,\n",
       "        0.95981958, 0.94620674, 0.98609879, 0.96666667, 0.95058415,\n",
       "        0.99167406, 0.96910677, 0.9523144 , 0.95572316, 0.94815143,\n",
       "        0.94029133, 0.98611358, 0.97156906, 0.96066992, 0.99534161,\n",
       "        0.97961402, 0.96243715, 0.99753032, 0.9807971 , 0.96217096,\n",
       "        0.96370157, 0.95459923, 0.94761165, 0.99125998, 0.97786897,\n",
       "        0.96455191, 0.99821059, 0.98522626, 0.96785714, 0.9989648 ,\n",
       "        0.98642413, 0.96748004]),\n",
       " 'split2_train_score': array([0.89057971, 0.88377699, 0.88251996, 0.91208222, 0.90249187,\n",
       "        0.8953638 , 0.9244824 , 0.91151287, 0.89770778, 0.93457557,\n",
       "        0.913916  , 0.89829932, 0.90499852, 0.9011535 , 0.8984546 ,\n",
       "        0.92846791, 0.91876664, 0.90928719, 0.94168885, 0.92439367,\n",
       "        0.91223011, 0.95147885, 0.92813517, 0.91265898, 0.9215247 ,\n",
       "        0.91385685, 0.91075865, 0.94545253, 0.93051612, 0.9225525 ,\n",
       "        0.9572168 , 0.93834664, 0.92596125, 0.96406389, 0.93995859,\n",
       "        0.92619787, 0.92639752, 0.91977226, 0.91470719, 0.9501331 ,\n",
       "        0.93884206, 0.92934043, 0.96240757, 0.94480923, 0.9327344 ,\n",
       "        0.96685892, 0.94642118, 0.93283052, 0.91083999, 0.90821503,\n",
       "        0.90353446, 0.93676427, 0.92851227, 0.92038598, 0.95424431,\n",
       "        0.93933747, 0.92531056, 0.96236321, 0.94093463, 0.92523662,\n",
       "        0.92495563, 0.92118456, 0.91626738, 0.95477669, 0.9440698 ,\n",
       "        0.93413191, 0.96864833, 0.95199645, 0.93715617, 0.97453416,\n",
       "        0.95448092, 0.93820615, 0.94015084, 0.93147737, 0.92678941,\n",
       "        0.97006063, 0.95800799, 0.94526028, 0.98188406, 0.96549098,\n",
       "        0.94875037, 0.98757764, 0.9666297 , 0.94954895, 0.94676131,\n",
       "        0.93936705, 0.93448684, 0.97856403, 0.96424874, 0.95251405,\n",
       "        0.98886424, 0.97185004, 0.95471754, 0.99183673, 0.97395741,\n",
       "        0.95672878, 0.91580893, 0.91291778, 0.90944247, 0.94724194,\n",
       "        0.93858326, 0.92945874, 0.96474416, 0.94826235, 0.93511535,\n",
       "        0.97349896, 0.95068027, 0.93510056, 0.931189  , 0.92572464,\n",
       "        0.92136202, 0.9649438 , 0.95190033, 0.94089027, 0.98044957,\n",
       "        0.96109879, 0.94526028, 0.98562555, 0.96182343, 0.94612541,\n",
       "        0.94731588, 0.93856847, 0.93277876, 0.97883023, 0.96547619,\n",
       "        0.95226264, 0.99179237, 0.97608696, 0.95763088, 0.99477965,\n",
       "        0.97582076, 0.95630731, 0.95769743, 0.94607365, 0.93674209,\n",
       "        0.98731884, 0.97447501, 0.96200828, 0.99571133, 0.98047915,\n",
       "        0.964685  , 0.99766341, 0.98172138, 0.96373114, 0.919336  ,\n",
       "        0.91694025, 0.91333185, 0.95555309, 0.94261313, 0.93291186,\n",
       "        0.97277433, 0.95391156, 0.93999556, 0.97932564, 0.95595238,\n",
       "        0.94068323, 0.93467169, 0.92891156, 0.92400917, 0.97295179,\n",
       "        0.95857734, 0.94685744, 0.98787341, 0.96759095, 0.95146406,\n",
       "        0.99114167, 0.96930642, 0.95113872, 0.95382283, 0.94382579,\n",
       "        0.93612836, 0.98594351, 0.97089618, 0.95948684, 0.99532683,\n",
       "        0.97939959, 0.96038894, 0.99723455, 0.98116681, 0.96189737,\n",
       "        0.96035936, 0.94882431, 0.94230997, 0.99222124, 0.97764715,\n",
       "        0.96523218, 0.99770778, 0.9838879 , 0.96793109, 0.99868382,\n",
       "        0.98603224, 0.96981662]),\n",
       " 'split3_train_score': array([0.87979986, 0.87912491, 0.8742902 , 0.89720188, 0.89225713,\n",
       "        0.88555163, 0.90877144, 0.89941749, 0.89189764, 0.91827212,\n",
       "        0.90317374, 0.89326222, 0.88954998, 0.89059909, 0.8851628 ,\n",
       "        0.9123883 , 0.90805981, 0.8963802 , 0.9265036 , 0.91321732,\n",
       "        0.90210996, 0.93443429, 0.91676815, 0.90316641, 0.90395874,\n",
       "        0.9015744 , 0.89452409, 0.92915205, 0.92160286, 0.90883013,\n",
       "        0.94528487, 0.92769944, 0.91361349, 0.9504644 , 0.92926944,\n",
       "        0.91409036, 0.9142004 , 0.90943906, 0.90081141, 0.93806582,\n",
       "        0.92827902, 0.91434713, 0.95018561, 0.93491116, 0.91918918,\n",
       "        0.95557055, 0.93507989, 0.92102329, 0.89552184, 0.8941646 ,\n",
       "        0.88774522, 0.92285006, 0.91510278, 0.90527196, 0.94361217,\n",
       "        0.9257993 , 0.91237363, 0.95288542, 0.93118425, 0.91464059,\n",
       "        0.91122181, 0.90712808, 0.90004842, 0.94526286, 0.9333485 ,\n",
       "        0.91989347, 0.96207064, 0.94460259, 0.92921808, 0.97245903,\n",
       "        0.94914384, 0.93024518, 0.93287163, 0.92329024, 0.91507344,\n",
       "        0.9661717 , 0.95091926, 0.93581354, 0.98140948, 0.96175517,\n",
       "        0.94161666, 0.98585535, 0.96197526, 0.94237231, 0.94207152,\n",
       "        0.93411148, 0.92617346, 0.97598785, 0.95994307, 0.94783062,\n",
       "        0.98789488, 0.96667058, 0.95576864, 0.99170983, 0.96884216,\n",
       "        0.95437472, 0.90083342, 0.89937347, 0.89301278, 0.93749358,\n",
       "        0.92652561, 0.91539624, 0.95844644, 0.94034012, 0.92431734,\n",
       "        0.96714745, 0.94505011, 0.92560122, 0.91597582, 0.91454521,\n",
       "        0.90656317, 0.95806494, 0.94413305, 0.92973897, 0.97515883,\n",
       "        0.95532112, 0.93703139, 0.98286209, 0.95931214, 0.93815386,\n",
       "        0.93574751, 0.93053864, 0.9219917 , 0.97607589, 0.95983302,\n",
       "        0.94529955, 0.98947955, 0.97019207, 0.95057444, 0.99232609,\n",
       "        0.97130721, 0.95113935, 0.94638534, 0.94019339, 0.93369331,\n",
       "        0.98633956, 0.96899623, 0.9574927 , 0.99409417, 0.98027233,\n",
       "        0.95926078, 0.99540739, 0.97870233, 0.96092615, 0.90375332,\n",
       "        0.9033865 , 0.89885258, 0.94719235, 0.9333485 , 0.92031899,\n",
       "        0.96675128, 0.94722169, 0.92888794, 0.9762593 , 0.95097795,\n",
       "        0.93019383, 0.9223952 , 0.92105997, 0.91213153, 0.9687688 ,\n",
       "        0.95097061, 0.93795578, 0.98432938, 0.96445498, 0.94524086,\n",
       "        0.98956759, 0.96499787, 0.94525553, 0.94200549, 0.93566681,\n",
       "        0.9271272 , 0.98507769, 0.96845333, 0.95386117, 0.99367599,\n",
       "        0.97753584, 0.9610362 , 0.99568618, 0.97829149, 0.96014849,\n",
       "        0.95253327, 0.94430179, 0.93576218, 0.99098352, 0.97751383,\n",
       "        0.96360395, 0.99693337, 0.98237055, 0.96405881, 0.99774038,\n",
       "        0.98369111, 0.9667146 ]),\n",
       " 'split4_train_score': array([0.88464191, 0.88250701, 0.88221355, 0.89836104, 0.89395184,\n",
       "        0.88858157, 0.91184541, 0.90363594, 0.89339427, 0.91988614,\n",
       "        0.9066292 , 0.89405455, 0.8940032 , 0.89166288, 0.88896307,\n",
       "        0.9152275 , 0.90506654, 0.89928543, 0.92841841, 0.91539624,\n",
       "        0.90364327, 0.93794844, 0.91800801, 0.90480243, 0.90569747,\n",
       "        0.90166244, 0.89846375, 0.93171247, 0.91703227, 0.91059088,\n",
       "        0.94528487, 0.92581398, 0.91666544, 0.95102197, 0.92873388,\n",
       "        0.91760451, 0.91528619, 0.90881546, 0.90490514, 0.93779437,\n",
       "        0.92510968, 0.91893974, 0.95156486, 0.93128696, 0.9239652 ,\n",
       "        0.95558523, 0.93457368, 0.92329024, 0.90034188, 0.89817763,\n",
       "        0.89334292, 0.92415594, 0.91716432, 0.91026074, 0.94174871,\n",
       "        0.92926944, 0.91680484, 0.95314953, 0.93410415, 0.91972474,\n",
       "        0.91454521, 0.90910892, 0.90490514, 0.946378  , 0.93571816,\n",
       "        0.92521239, 0.96535736, 0.94604786, 0.93358326, 0.97310463,\n",
       "        0.95068449, 0.93362728, 0.9350212 , 0.92792687, 0.92143413,\n",
       "        0.96873212, 0.95258463, 0.9422696 , 0.98145349, 0.96340587,\n",
       "        0.94710431, 0.9855619 , 0.96563614, 0.94711898, 0.94671548,\n",
       "        0.93655452, 0.93277625, 0.97619327, 0.9600091 , 0.95182897,\n",
       "        0.98629554, 0.96926768, 0.95474887, 0.9908588 , 0.97149795,\n",
       "        0.95408859, 0.90672458, 0.904575  , 0.8988966 , 0.9399733 ,\n",
       "        0.9291814 , 0.92078852, 0.95872522, 0.94203483, 0.92718589,\n",
       "        0.96823324, 0.94444118, 0.92830103, 0.92281338, 0.91811072,\n",
       "        0.91272578, 0.95978167, 0.94603319, 0.93568148, 0.97731575,\n",
       "        0.95888662, 0.94090502, 0.98363975, 0.96101419, 0.94163867,\n",
       "        0.94352413, 0.9356448 , 0.92808827, 0.97735976, 0.96255484,\n",
       "        0.9507065 , 0.99013983, 0.97143193, 0.9537878 , 0.99267824,\n",
       "        0.97242968, 0.9541106 , 0.95299547, 0.94254838, 0.93571816,\n",
       "        0.9854005 , 0.97292856, 0.95988438, 0.99446833, 0.9776092 ,\n",
       "        0.95966428, 0.99555412, 0.97986882, 0.96157176, 0.91007733,\n",
       "        0.90904289, 0.90142033, 0.94799935, 0.93473508, 0.92371576,\n",
       "        0.96799847, 0.94821945, 0.9305093 , 0.97757986, 0.95146215,\n",
       "        0.93228471, 0.93001775, 0.92417795, 0.91570437, 0.96873212,\n",
       "        0.95389785, 0.94116913, 0.98476956, 0.9634132 , 0.94722903,\n",
       "        0.98980236, 0.9673602 , 0.94791132, 0.9503837 , 0.93891685,\n",
       "        0.93341452, 0.98532713, 0.97026543, 0.95566593, 0.99373469,\n",
       "        0.97872434, 0.95884994, 0.99602365, 0.98022097, 0.96080877,\n",
       "        0.9586812 , 0.94961337, 0.93965049, 0.99201796, 0.97724972,\n",
       "        0.96314175, 0.99706543, 0.98426335, 0.96769034, 0.99784309,\n",
       "        0.98542984, 0.96657521]),\n",
       " 'mean_train_score': array([0.88448551, 0.88154851, 0.87911796, 0.90353287, 0.89740299,\n",
       "        0.88975566, 0.91655608, 0.90599371, 0.89433877, 0.92527584,\n",
       "        0.90907535, 0.89551571, 0.89731401, 0.89555177, 0.89122121,\n",
       "        0.91919959, 0.91156345, 0.9021071 , 0.93302374, 0.91883422,\n",
       "        0.9067762 , 0.94212369, 0.92198185, 0.90799424, 0.91210871,\n",
       "        0.90791445, 0.90211132, 0.93705755, 0.9246451 , 0.91558045,\n",
       "        0.94931921, 0.93225992, 0.92017291, 0.95519671, 0.93407922,\n",
       "        0.92047089, 0.92088993, 0.91470089, 0.90910101, 0.94415163,\n",
       "        0.93234736, 0.92281679, 0.95550981, 0.93834758, 0.92671872,\n",
       "        0.95996408, 0.94001619, 0.92705998, 0.90285657, 0.90018983,\n",
       "        0.89419012, 0.92882741, 0.92032358, 0.91180071, 0.94677641,\n",
       "        0.93138642, 0.918227  , 0.95594375, 0.93526117, 0.91989732,\n",
       "        0.91680972, 0.91266177, 0.9067257 , 0.94811226, 0.93709934,\n",
       "        0.92665205, 0.96513215, 0.94737735, 0.9327052 , 0.97295627,\n",
       "        0.95062375, 0.9332285 , 0.93527629, 0.92749719, 0.92092322,\n",
       "        0.96817568, 0.95330504, 0.94101   , 0.98142057, 0.96259949,\n",
       "        0.94559246, 0.98638786, 0.96404934, 0.94620054, 0.94580827,\n",
       "        0.93583388, 0.93064235, 0.97723451, 0.96165533, 0.95005023,\n",
       "        0.98798065, 0.96881439, 0.95415319, 0.99136288, 0.97083348,\n",
       "        0.95428953, 0.90820341, 0.90560484, 0.9006667 , 0.94111154,\n",
       "        0.93112661, 0.9211707 , 0.96041895, 0.94270244, 0.92811786,\n",
       "        0.96952955, 0.94624934, 0.92881062, 0.92364071, 0.91929368,\n",
       "        0.9131302 , 0.9612771 , 0.94666235, 0.93516779, 0.97806812,\n",
       "        0.95778091, 0.94077066, 0.98455887, 0.96060475, 0.94154709,\n",
       "        0.94300162, 0.93448336, 0.92779919, 0.97824702, 0.96286858,\n",
       "        0.94904445, 0.99073754, 0.97251143, 0.95427233, 0.9938169 ,\n",
       "        0.97368822, 0.95371192, 0.95178536, 0.9426501 , 0.93539664,\n",
       "        0.98640657, 0.9718085 , 0.95850795, 0.99473261, 0.97959464,\n",
       "        0.96043541, 0.99648719, 0.98044833, 0.9614916 , 0.9114677 ,\n",
       "        0.90962134, 0.90435213, 0.94935038, 0.93603316, 0.92525031,\n",
       "        0.96906323, 0.94882943, 0.93250352, 0.97780569, 0.9518811 ,\n",
       "        0.93352943, 0.92919895, 0.92383759, 0.91703272, 0.96939903,\n",
       "        0.95409259, 0.94117399, 0.9851197 , 0.96470407, 0.94709202,\n",
       "        0.99049836, 0.96720217, 0.94780521, 0.94863312, 0.93941836,\n",
       "        0.93197968, 0.9850718 , 0.96926845, 0.95579036, 0.99449042,\n",
       "        0.97813018, 0.9593987 , 0.99668713, 0.97934816, 0.9602163 ,\n",
       "        0.9574455 , 0.94770347, 0.93918635, 0.99127495, 0.97698347,\n",
       "        0.9636174 , 0.99741407, 0.98334305, 0.96614723, 0.99830185,\n",
       "        0.98514185, 0.9664524 ]),\n",
       " 'std_train_score': array([0.00623453, 0.00618853, 0.005869  , 0.00662605, 0.00552861,\n",
       "        0.00513018, 0.00601039, 0.00505188, 0.00470009, 0.00610274,\n",
       "        0.00507124, 0.00464432, 0.00757983, 0.00682773, 0.00674548,\n",
       "        0.00689915, 0.00569552, 0.005746  , 0.00623575, 0.00535966,\n",
       "        0.00519511, 0.00639067, 0.00584294, 0.0053494 , 0.00819645,\n",
       "        0.00751101, 0.00774719, 0.00700108, 0.00657767, 0.00635758,\n",
       "        0.00471748, 0.00622864, 0.00619767, 0.00529994, 0.00599198,\n",
       "        0.00611086, 0.00791422, 0.00636731, 0.00705415, 0.0065457 ,\n",
       "        0.00642217, 0.00697437, 0.00502016, 0.00589167, 0.0058946 ,\n",
       "        0.00473039, 0.0056819 , 0.00544488, 0.00754333, 0.0078113 ,\n",
       "        0.0071399 , 0.00687977, 0.00680445, 0.00680103, 0.00521214,\n",
       "        0.00605065, 0.00588712, 0.00429309, 0.00489652, 0.00521394,\n",
       "        0.00775521, 0.00838873, 0.00828532, 0.00564117, 0.00571827,\n",
       "        0.00643187, 0.00279107, 0.00418435, 0.00527923, 0.00262759,\n",
       "        0.00354091, 0.00555503, 0.00569594, 0.0063116 , 0.00746714,\n",
       "        0.00216299, 0.00429188, 0.00566125, 0.00119506, 0.00292951,\n",
       "        0.00484708, 0.00096409, 0.00269139, 0.00476488, 0.00475074,\n",
       "        0.00500105, 0.00634568, 0.00156208, 0.00204303, 0.00405944,\n",
       "        0.00105668, 0.00249013, 0.00263406, 0.00084526, 0.00248321,\n",
       "        0.00277875, 0.00818338, 0.00848928, 0.00876724, 0.00601155,\n",
       "        0.00598736, 0.00667082, 0.00315557, 0.00457831, 0.00590861,\n",
       "        0.00253988, 0.00422528, 0.00571918, 0.00733995, 0.00730265,\n",
       "        0.00821577, 0.00354281, 0.00516576, 0.00605396, 0.00186304,\n",
       "        0.0029393 , 0.00473442, 0.00110107, 0.0021142 , 0.00489128,\n",
       "        0.00638176, 0.0060622 , 0.00690205, 0.0016418 , 0.00198775,\n",
       "        0.00414875, 0.00106958, 0.00200813, 0.00355006, 0.00113945,\n",
       "        0.00164482, 0.00322058, 0.00549147, 0.00458365, 0.00539554,\n",
       "        0.00066497, 0.0024046 , 0.00311153, 0.00058051, 0.00141772,\n",
       "        0.00267359, 0.00087264, 0.0011142 , 0.0021196 , 0.00827394,\n",
       "        0.00859882, 0.00858826, 0.00635009, 0.00631345, 0.00674096,\n",
       "        0.00294261, 0.00410724, 0.00598703, 0.00149059, 0.00390081,\n",
       "        0.00600995, 0.00718369, 0.00654398, 0.00737915, 0.00352728,\n",
       "        0.00469547, 0.00498265, 0.00178936, 0.00223012, 0.00380993,\n",
       "        0.00079798, 0.00183254, 0.00366354, 0.00598318, 0.00614824,\n",
       "        0.00622367, 0.00115238, 0.00230194, 0.00409275, 0.00073107,\n",
       "        0.00155556, 0.00280654, 0.00071102, 0.00183274, 0.00220505,\n",
       "        0.00454892, 0.00461736, 0.00576979, 0.00083007, 0.00118991,\n",
       "        0.00126107, 0.00047778, 0.00150206, 0.00207568, 0.00047154,\n",
       "        0.00106251, 0.00265617])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gsearch1.grid_scores_\n",
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.3,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 5}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8849672823827699"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784753\n",
      "F1: 0.661972\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2158e780>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
      "       min_child_weight=3, missing=nan, n_estimators=140, n_jobs=1,\n",
      "       nthread=4, objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=27, silent=None,\n",
      "       subsample=0.8, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
